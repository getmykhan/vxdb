---
title: OpenAI Embeddings
description: "Generate embeddings with OpenAI's API and store them in vxdb."
---

## Setup

```bash
pip install vxdb openai
```

You'll need an OpenAI API key from [platform.openai.com/api-keys](https://platform.openai.com/api-keys).

## Available models

| Model | Dimensions | Price | Notes |
|---|---|---|---|
| `text-embedding-3-small` | 1536 | $0.02/1M tokens | Recommended default |
| `text-embedding-3-large` | 3072 | $0.13/1M tokens | Higher quality |
| `text-embedding-ada-002` | 1536 | $0.10/1M tokens | Legacy |

## Create an embedding helper

```python
import os
from openai import OpenAI

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIM = 1536

def get_embeddings(texts: list[str]) -> list[list[float]]:
    response = client.embeddings.create(
        input=texts,
        model=EMBEDDING_MODEL,
    )
    return [item.embedding for item in response.data]
```

<Tip>
The OpenAI API accepts batches of up to 2048 texts. Always batch your calls for cost and speed.
</Tip>

## Index documents

```python
import vxdb

documents = [
    {"id": "ml-intro", "text": "Machine learning enables systems to learn from data.", "metadata": {"topic": "ml"}},
    {"id": "rag", "text": "RAG combines retrieval with generative models to reduce hallucination.", "metadata": {"topic": "rag"}},
    {"id": "vec-db", "text": "Vector databases store embeddings for fast similarity search.", "metadata": {"topic": "infra"}},
]

texts = [doc["text"] for doc in documents]
vectors = get_embeddings(texts)

# Use path= for persistent storage: vxdb.Database(path="./my_data")
db = vxdb.Database()
collection = db.create_collection("openai_docs", dimension=EMBEDDING_DIM, metric="cosine")

collection.upsert(
    ids=[doc["id"] for doc in documents],
    vectors=vectors,
    metadata=[doc["metadata"] for doc in documents],
    documents=texts,
)
```

## Search

### Semantic search

```python
query = "How do I make my LLM more accurate with external knowledge?"
query_vector = get_embeddings([query])[0]

results = collection.query(vector=query_vector, top_k=3)
```

### Filtered search

```python
results = collection.query(
    vector=query_vector,
    top_k=3,
    filter={"topic": {"$eq": "ml"}},
)
```

### Hybrid search

```python
results = collection.hybrid_query(
    vector=query_vector,
    query="retrieval augmented generation hallucination",
    top_k=3,
    alpha=0.5,
)
```

## Reusable EmbeddingFunction wrapper

For cleaner code, wrap OpenAI into vxdb's pluggable interface:

```python
from vxdb.embedding import EmbeddingFunction

class OpenAIEmbedding(EmbeddingFunction):
    def __init__(self, model: str = "text-embedding-3-small"):
        self.client = OpenAI()
        self.model = model

    def embed(self, texts: list[str]) -> list[list[float]]:
        response = self.client.embeddings.create(input=texts, model=self.model)
        return [item.embedding for item in response.data]

embedder = OpenAIEmbedding()
vecs = embedder.embed(["This is a test sentence."])
```
