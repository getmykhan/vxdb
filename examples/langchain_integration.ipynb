{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain + vxdb\n",
        "\n",
        "This notebook shows how to use **LangChain's embedding abstractions** with vxdb. LangChain gives you a unified interface to swap between providers (OpenAI, Hugging Face, Cohere, Ollama, etc.) without changing your vector store code.\n",
        "\n",
        "**What this notebook covers:**\n",
        "1. Using LangChain's `OpenAIEmbeddings` to generate vectors\n",
        "2. Using LangChain's `HuggingFaceEmbeddings` for local, free embeddings\n",
        "3. A reusable pattern for any LangChain embedding provider\n",
        "4. Building a simple RAG (Retrieval-Augmented Generation) pipeline\n",
        "\n",
        "**Prerequisites:**\n",
        "```bash\n",
        "pip install vxdb langchain-openai langchain-huggingface\n",
        "```\n",
        "\n",
        "> **Tip:** You only need the provider package you want. For example, `langchain-openai` for OpenAI, `langchain-huggingface` for local models."
      ],
      "id": "6881a0b1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install vxdb langchain-openai langchain-huggingface -q"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f622f354"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option A: LangChain + OpenAI Embeddings\n",
        "\n",
        "Requires `OPENAI_API_KEY` environment variable."
      ],
      "id": "eef082e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# LangChain handles API key from OPENAI_API_KEY env var automatically\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# LangChain provides two methods:\n",
        "# - embed_documents(texts) → for indexing (batch)\n",
        "# - embed_query(text)      → for searching (single)\n",
        "\n",
        "test_vec = embeddings.embed_query(\"hello world\")\n",
        "print(f\"OpenAI via LangChain: {len(test_vec)} dimensions\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "639b6e07"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option B: LangChain + Hugging Face (Local, Free)\n",
        "\n",
        "No API key needed. Runs entirely on your machine."
      ],
      "id": "328ca2d9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "test_vec = embeddings.embed_query(\"hello world\")\n",
        "EMBEDDING_DIM = len(test_vec)\n",
        "print(f\"HuggingFace via LangChain: {EMBEDDING_DIM} dimensions (local, free)\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "64b94e67"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Index documents with LangChain embeddings + vxdb\n",
        "\n",
        "The pattern is always the same regardless of which LangChain embedding provider you chose above:\n",
        "1. Call `embed_documents()` to batch-embed your texts\n",
        "2. `upsert()` into a vxdb collection"
      ],
      "id": "0c9aff32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import vxdb\n",
        "\n",
        "# Sample knowledge base\n",
        "docs = [\n",
        "    {\"id\": \"k8s\",     \"text\": \"Kubernetes orchestrates containerized applications across clusters of machines, handling scaling, networking, and self-healing.\",        \"source\": \"infra\"},\n",
        "    {\"id\": \"docker\",  \"text\": \"Docker packages applications into lightweight, portable containers that run consistently across development, staging, and production.\",    \"source\": \"infra\"},\n",
        "    {\"id\": \"terraform\",\"text\": \"Terraform is an infrastructure-as-code tool that lets you define cloud resources in declarative configuration files.\",                    \"source\": \"infra\"},\n",
        "    {\"id\": \"react\",   \"text\": \"React is a JavaScript library for building user interfaces with a component-based architecture and virtual DOM for efficient rendering.\",  \"source\": \"frontend\"},\n",
        "    {\"id\": \"fastapi\", \"text\": \"FastAPI is a modern Python web framework for building APIs, featuring automatic OpenAPI docs and async support out of the box.\",          \"source\": \"backend\"},\n",
        "    {\"id\": \"postgres\",\"text\": \"PostgreSQL is a powerful open-source relational database known for reliability, feature richness, and support for complex queries and JSON.\",\"source\": \"database\"},\n",
        "    {\"id\": \"redis\",   \"text\": \"Redis is an in-memory data store used as a cache, message broker, and real-time database, offering sub-millisecond latency.\",              \"source\": \"database\"},\n",
        "]\n",
        "\n",
        "texts = [d[\"text\"] for d in docs]\n",
        "\n",
        "# Embed with LangChain (works with any provider)\n",
        "vectors = embeddings.embed_documents(texts)\n",
        "\n",
        "# Insert into vxdb\n",
        "db = vxdb.Database()\n",
        "collection = db.create_collection(\"devtools\", dimension=EMBEDDING_DIM, metric=\"cosine\")\n",
        "\n",
        "collection.upsert(\n",
        "    ids=[d[\"id\"] for d in docs],\n",
        "    vectors=vectors,\n",
        "    metadata=[{\"source\": d[\"source\"]} for d in docs],\n",
        "    documents=texts,\n",
        ")\n",
        "\n",
        "print(f\"Indexed {collection.count()} documents\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "78b1fcad"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Search"
      ],
      "id": "374d8f32"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "query = \"How do I deploy containers to production?\"\n",
        "\n",
        "# embed_query for single queries (some providers optimize this differently)\n",
        "query_vec = embeddings.embed_query(query)\n",
        "\n",
        "# Vector search\n",
        "print(f\"Q: {query}\\n\")\n",
        "print(\"Vector search:\")\n",
        "for r in collection.query(vector=query_vec, top_k=3):\n",
        "    print(f\"  → {r['id']:>12}  score={r['score']:.4f}  source={r['metadata']['source']}\")\n",
        "\n",
        "# Filtered: only infrastructure tools\n",
        "print(\"\\nFiltered (source=infra only):\")\n",
        "for r in collection.query(vector=query_vec, top_k=3, filter={\"source\": {\"$eq\": \"infra\"}}):\n",
        "    print(f\"  → {r['id']:>12}  score={r['score']:.4f}\")\n",
        "\n",
        "# Hybrid\n",
        "print(\"\\nHybrid search:\")\n",
        "for r in collection.hybrid_query(vector=query_vec, query=query, top_k=3, alpha=0.5):\n",
        "    print(f\"  → {r['id']:>12}  score={r['score']:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "bac454e1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Simple RAG pipeline\n",
        "\n",
        "Retrieve relevant context from vxdb, then pass it to an LLM for answer generation.\n",
        "\n",
        "> This cell requires `OPENAI_API_KEY` and uses `langchain-openai`. Swap in any LangChain LLM provider."
      ],
      "id": "d1eb41c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "\n",
        "def rag_query(question: str, top_k: int = 3) -> str:\n",
        "    \"\"\"Retrieve context from vxdb, then ask the LLM.\"\"\"\n",
        "    # 1. Embed the question\n",
        "    q_vec = embeddings.embed_query(question)\n",
        "\n",
        "    # 2. Retrieve relevant documents\n",
        "    results = collection.query(vector=q_vec, top_k=top_k)\n",
        "\n",
        "    # 3. Build context string from retrieved docs\n",
        "    context_parts = []\n",
        "    for r in results:\n",
        "        original_text = next(d[\"text\"] for d in docs if d[\"id\"] == r[\"id\"])\n",
        "        context_parts.append(f\"[{r['id']}] {original_text}\")\n",
        "    context = \"\\n\".join(context_parts)\n",
        "\n",
        "    # 4. Ask the LLM with the retrieved context\n",
        "    prompt = f\"\"\"Answer the question based on the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "\n",
        "# Try it\n",
        "answer = rag_query(\"What tool should I use for infrastructure as code?\")\n",
        "print(answer)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3ecea122"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Swapping providers\n",
        "\n",
        "The beauty of LangChain is that you can swap embedding providers with one line change:\n",
        "\n",
        "```python\n",
        "# OpenAI (API key required)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Hugging Face (local, free)\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Cohere (API key required)\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
        "\n",
        "# Ollama (local, free, needs ollama running)\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "\n",
        "# Google (API key required)\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "```\n",
        "\n",
        "The rest of your vxdb code stays exactly the same."
      ],
      "id": "fa16b4e8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}